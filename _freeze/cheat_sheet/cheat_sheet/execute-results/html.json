{
  "hash": "0a9fdc2222797a12475be4fa08cb8926",
  "result": {
    "markdown": "---\ntitle: Basic R coding overview\nformat:\n  html:\n    number-sections: true\n    toc-location: left\n    code-overflow: wrap\nexecute: \n  eval: false\n  echo: true\n---\n\n\nThis document aligns with the Stata cheat sheet.\n\n# Tips before you start\n\n-   There is a lot of documentation and support available at your fingertips both within R and outside of R on blogs, other educational websites, Twitter, YouTube and more.\n\n    -   Within R, use `help(function_name)` to get an overview, more options and examples for a specific function. You can also use a question mark.\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        help(function_name)\n        ?function_name\n        ```\n        :::\n\n\n    -   You can also directly search for a package or function from the help panel in RStudio.\n\n-   R is case sensitive (e.g. if a variable is 'ID', you must refer to it as 'ID', not 'id').\n\n-   It is best to save and write your code in a R script (.R) or an RMarkdown (.Rmd) or Quarto (.qmd) file.\n\n-   Notes/comments can be added for easy annotation of the program. A pound sign/hash at the beginning of the line denotes a comment that can be used interactively. For example:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # This is a note that\n    # R will not read\n    ```\n    :::\n\n\n-   If you start to run a function and need to stop it, you can press the stop sign at the top right of the console.\n\n# Getting started with R\n\nOpen RStudio. If you have used R previously, an old workspace may still be active. However, you always want to start with a fresh session. Go to Tools -\\> Global Options, and under General, change these settings:\n\n![](../img/Screenshot%202023-05-24%20at%205.44.10%20PM.png)\n\nNow, you can quit RStudio if something goes wrong. You can also go to Session -\\> Restart R to clear your session.\n\n## Create and execute an R script\n\nIn RStudio, you can run R code directly in the console. This is helpful for running quick one-off code. To save your code, write it as an .R script, which allows one to run the same commands in the future to reproduce your work. To open a new .R script, one can:\n\n1.  Click on the \"New File\" icon in the top left of RStudio, then select \"R Script\".\n2.  File -\\> New File -\\> R Script\n3.  {{< kbd mac=Shift-Command-N win=Shift-Control-N linux=Shift-Ctrl-N >}}\n\n## Installing packages {.unnumbered}\n\nTo utilize certain functions in R, additional packages often need to be downloaded into R. To install a package, run:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"package_name\")\n```\n:::\n\n\nYou can also click on the \"Packages\" tab in the bottom right window in RStudio. Next click \"install\" and a small window will pop up, enter the package's name and hit \"install\".\n\nWhen a function belongs to a package, you can use that function by running `package_name::function_name()` or by running `library(package_name)` at some point before `function_name()`.\n\n## Read in a Dataset (.rds, .dta, .xlsx, .csv, .sas7bdat)\n\nDatasets can be in various formats, such as \".rds\" (R), \".dta\" (Stata), \".xlsx\" (Excel), \".csv\" (Comma Separated Value), and \".sas7bdat\" (SAS) files. Some of these datasets require additional packages to download into R. \n\nYou will need to include the path to the dataset file in your code. To determine the location of a dataset stored on a Windows computer, you can right-click and click on 'Properties' and copy and paste the location path. On a Mac, right-click the file, then press {{< kbd Option >}} so that \"Copy filename as Pathname\" appears in the menu.\n\nUnlike Stata, you can have multiple datasets open in the same R session. When reading in a dataset, you must store it as an object so that you can refer to it later. This is done in R using the \"assignment arrow\" `<-`.\n\nReading in datasets can also be done from \"File\" -\\> \"Import Dataset\". This will bring up a window with options that will help you write the code that can be saved in an R script to use again. \nThe \"files\" tab in the bottom right window of RStudio may also be helpful to import datasets. It operates similarly to Windows Explorer or the Finder on a Mac.\n\n### Reading an R \".rds\"-file\n\nThe `readRDS()` function is part of \"base R\". This means it does not need a separate package. The `{readr}` package includes many functions for reading in data that do the same things as the base R packages, often with better defaults.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- readRDS(\"filepath/filename.rds\")\n```\n:::\n\n\nor\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- readr::read_rds(\"filepath/filename.rds\")\n```\n:::\n\n\n### Reading a Stata \".dta\"-file\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- haven::read_stata(\"filepath/filename.dta\")\n```\n:::\n\n\n### Importing an Excel file\n\nBecause data in Excel can take many different forms (different sheets, extra rows with information, etc.), there are many arguments to this function to make sure you are reading in the right data. It will default to the first sheet, with the first row used as column names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- readxl::read_excel(\"filepath/filename.xlsx\")\n```\n:::\n\n\n### Importing a \".csv\"-file\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- read.csv(\"filepath/filename.csv\")\n```\n:::\n\n\nor\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- readr::read_csv(\"filepath/filename.csv\")\n```\n:::\n\n\n### Reading in a SAS file\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhaven::read_sas(\"filepath/filename.sas7bdat\",\n                \"filepath/catalog.sas7bcat\")\n```\n:::\n\n\n## Get to know your data\n\nThere are a number of ways to summarize your dataset. \nThe `summary()` function will print summary statistics for each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dataset)\n```\n:::\n\n\nThe `glimpse()` function tells you the number of rows and columns, what type of variable each column contains, and shows you the first few observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::glimpse(dataset)\n```\n:::\n\n\nTo see what the entire dataset looks like, you can click on the name of the dataset in the Environment pane. This will open up a new tab with the dataset. You can also bring up this tab with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nView(dataset)\n```\n:::\n\n\nor\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble::view(dataset)\n```\n:::\n\n\nTo compactly list the variable names:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(dataset)\n```\n:::\n\n\nTo count the number of observations (i.e. rows of data) in a dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(dataset)\n```\n:::\n\n\nTo count the number of observations in a subset of the data:\n\nCode for character variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(dataset[dataset$varname == \"Male\",])\n```\n:::\n\n\nCode for numeric variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(dataset[dataset$varname == 1,])\n```\n:::\n\n\nYou can also count the number of observations for each value of `varname` with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::count(dataset, varname)\n```\n:::\n\n\n# Basic data management\n\n## Generate quadratic term\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset$newvar <- dataset$oldvar * dataset$newvar\n```\n:::\n\n\nor\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- mutate(dataset, newvar = oldvar * oldvar)\n```\n:::\n\n\nYou can also create a quadratic term directly in a model itself, but it must be wrapped in `I()`: `var + I(var^2) + other_vars` .\n\n## Generate a categorical variable based on cut-points\n\nThere are a couple ways to create a categorical variable `'newvar'` based on cutpoint values of a continuous variable `'oldvar'` at cutpoints `x` and `y`.\n\n1)  One can use the `cut()` function to create a new categorical variable with cutpoints at the `breaks =` values. For example, say you have a variable with a minimum of 0 and a maximum of 100 with some missing values and you would like to make three categories defined by the values (0,x\\] (category 1), (x,y\\] (category 2), and (y,100\\] (category 3). For the `breaks =`argument, you will want to supply the left-hand breaks: a value lower than the minimum, x, y and then a value larger than the maximum. (You can use `-Inf` and `Inf` if you don't know what the minimum and maximum are.) You can also use the `include.lowest =` and the `right =` arguments to change how the intervals are created.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dataset$newvar <- cut(dataset$newvar, \n                          breaks = c(-Inf, x, y, Inf))\n    ```\n    :::\n\n\n2)  You can also use `case_when()` if you want to be more explicit. For example, to generate a new categorical variable based on a set of cutpoints (ex. 40, 60, 80):\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(dplyr)\n    dataset <- mutate(dataset, newvar = case_when(\n      oldvar < 40 ~ 0,\n      oldvar >= 40 & oldvar < 60 ~ 1,\n      oldvar >= 60 & oldvar < 80 ~ 2,\n      oldvar >= 80 ~ 3\n    ))\n    ```\n    :::\n\n\n    Values of `oldvar` that don't meet any of the conditions will be assigned `NA`.\n\n## Generate quantiles\n\nUse the `quantile()` function to generate a categorical variable based on quantile cutoffs. In the code below, set the number of desired quantiles (here, tertiles) by changing the value of `nquantiles`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnquantiles <- 3\ndataset$newvar <- cut(dataset$oldvar, \n                      breaks = quantile(dataset$oldvar, \n                                        probs = seq(0, 1, \n                                                    length.out = nquantiles + 1)), \n                      include.lowest=TRUE)\n```\n:::\n\n\n## Generate an indicator/dummy variable\n\n1)  You generally don't need to create indicator/dummy variables. In models, R will treat any factor variable as a series of indicator variables. If `varname` has numeric values, turn it into a factor variable (see next section):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset$varname <- factor(dataset$varname)\n```\n:::\n\n\nand then use `varname` directly in a model. \n\n2) One could also code the indicator/dummy variables manually using `case_when()`. \n3) In some scenarios, it may be helpful to generate the indicator variables used in a model with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewvars <- as.data.frame(model.matrix(~varname + othervar, \n                                      data = dataset))\n```\n:::\n\n\nNote that this will also create an intercept (a column of ones) and will only contain the variables `varname` and `othervar`, no other variables from the dataset.\n\n## Label a categorical variable\n\nTo create a factor (categorical) variable, use the `factor()` function, with arguments `levels =` for the unique values and `labels =` for the names of those values. The labels wil be assigned to the level in the same order they are specified. For example, if `oldvar` has values 0, 1, and 2, which should be named \"small\" (0), \"medium\" (1), \"large\" (2):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset$newvar <- factor(dataset$oldvar, levels = c(0, 1, 2), \n                         labels = c(\"small\", \"medium\", \"large\"))\n```\n:::\n\n\nThe reference level is always the first level (e.g. \"small\"). In models, R will treat character variables as factor variables, and the levels will be in alphabetical order. For example, if the variable in the example above was already labeled, but not a factor variable, putting it in a model or using the `factor()` function would result in levels \"large\", \"medium\", and \"small\" (\"large\" being the reference).\n\nTo determine whether a variable is already a factor variable, or whether it is a character variable (or another type), use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(dataset$varname)\n```\n:::\n\n\nTo see the levels in order of a factor variable, use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(dataset$varname)\n```\n:::\n\n\n\nThe `{forcats}` package is designed to make working with categorical variables easier. If a variable has levels \"large\", \"medium\", and \"small\" and you want to change the order, you can use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset$newvar <- forcats::fct_relevel(dataset$oldvar, \n                                       \"small\", \"medium\", \"large\"))\n```\n:::\n\n\nTo rename the levels \"sm\", \"md\", and \"lg\":\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- forcats::fct_recode(dataset$oldvar, \n                               \"sm\" = \"small\", \"md\" = \"medium\", \"lg\" = \"large\"))\n```\n:::\n\n\nIf the variable is numeric, you need to turn it into a character or factor variable before using `fct_recode()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- forcats::fct_recode(as.character(dataset$oldvar), \n                               \"sm\" = \"0\", \"md\" = \"1\", \"lg\" = \"2\"))\n```\n:::\n\n\n## Sorting data\n\nThe default of the `arrange()` function from the `{dplyr}` package is in ascending order.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- dplyr::arrange(dataset, varname)\n```\n:::\n\n\nTo sort in descending order:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ndataset <- arrange(dataset, desc(varname))\n```\n:::\n\n\nYou can sort on multiple variables to arrange observations into ascending order based on `varname1`, and within each `varname1` category one can sort into descending order by `varname2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ndataset <- arrange(dataset, varname1, desc(varname2))\n```\n:::\n\n\n## Merge two datasets\n\nTo merge two datasets, use a `_join()` function from the `{dplyr}` package: `left_join()`, `right_join()`, `full_join()`. The functions differ by how they treat rows without a match in the \"left-hand side\" dataset and the \"right-hand side\" dataset. \n\nThis will keep all observations in `dataset1` and merge only with observations from `dataset2` with a matching `id` in `dataset1`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ndataset3 <- left_join(dataset1, dataset2, join_by(id))\n```\n:::\n\n\n## Keeping and dropping observations\n\nTo create a dataset that is a subset of another dataset, use `filter()`. This code will retain all observations with both the age and sex requirement:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset2 <- dplyr::filter(dataset1, age > 35, sex == \"F\")\n```\n:::\n\n\nYou can also explicitly use logic operators such as `&` (and) and `|` (or). To retain observations that are *either* older than 35 *or* female (or both):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset2 <- dplyr::filter(dataset1, age > 35 | sex == \"F\")\n```\n:::\n\n\n# Descriptives, statistical tests, and visualization tools\n\n## Categorical data: frequencies, tests, and association measures\n\n### One-way frequency table\n\nOne can get a frequency table using the `count()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::count(dataset, varname)\n```\n:::\n\n\nThe `tabyl()` function from the `{janitor}` package is also helpful for frequency tables because it will also add percentages by default to a one-way frequency table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njanitor::tabyl(dataset, varname)\n```\n:::\n\n\nFor a binary variable, you can use `prop.test()` or `binom.test()` (exact intervals) to get confidence intervals for the proportion of 1's (or the \"higher\" level for a factor variable):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(table(dataset$binary_var))\n```\n:::\n\n\n### RxC frequency tables\n\nOne can easily extend the functions above to cross-tabulate two categorical variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::count(dataset, varname1, varname2)\njanitor::tabyl(dataset, varname1, varname2)\n```\n:::\n\n\nTo get proportions when using `tabyl()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(janitor)\ntabyl(dataset, varname1, varname2) |> adorn_percentages()\n```\n:::\n\n\n### Two-sample test of proportions\n\nTo compare proportions of `varname2` stratified by `varname1` and get a confidence interval for the difference in proportions;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(table(dataset$varname1, dataset$varname2))\n```\n:::\n\n\n### Estimate measures of association\n\nThe `{epitools}` package contains functions for estimating various simple epidemiologic quantities. For example, to estimate a risk ratio (and the corresponding proportions):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepitools::epitab(dataset$exposure, dataset$outcome, \n                 method = \"riskratio\")\n```\n:::\n\n\n### Stratified frequency tables\n\nYou can estimate any summary statistic of one variable (ex. `varname1` or `varname2`) within different levels of a categorical variable (ex. `categorical_variable`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::summarize(dataset, mean_1 = mean(varname1), \n                 sd_2 = sd(varname2), .by = categorical_variable)\n```\n:::\n\n\nFor two-way frequency tables (`varname1` x `varname2`) stratified by a third variable (`categoricalvariable`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\njanitor::tabyl(dataset, varname1, varname2, categorical_variable)\n```\n:::\n\n\n## Continuous data: summary statistics, visualization, and statistical tests\n\n### Measures of location and variance\n\nUse the `summary` command to obtain summary statistics for continuous data (mean, minimum, maximum, median, IQR):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dataset$varname)\n```\n:::\n\n\nYou can also estimate arbitrary summary statistics using `dplyr::summarize()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::summarize(dataset, mean = mean(varname), sd = sd(varname))\n```\n:::\n\n\nOne can estimate the standard errors and 95% CI of the mean using the `ci` or `ci means` command.\n`t.test()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(dataset$varname)\n```\n:::\n\n\n### Histogram\n\nA histogram graphic for continuous data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(dataset$age)\n```\n:::\n\n\nWith the `{ggplot2}` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dataset, aes(x = age)) + \n    geom_histogram() +\n    labs(title = \"Histogram of age\", \n         x = \"Age\")\n```\n:::\n\n\n### Box plots\n\nTo show a box-and-whisker plot of age and a box plot of age by a categorical variable (ex. sex):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(dataset$age, dataset$sex)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dataset, aes(y = age, x = sex)) + \n    geom_boxplot() +\n    labs(title = \"Boxplot of age by sex\", \n         x = \"Sex\", y = \"Age\")\n```\n:::\n\n\n### Scatterplot\n\nScatterplot of two continuous variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(dataset$age, dataset$bmi)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dataset, aes(y = age, x = bmi)) + \n    geom_point() +\n    labs(x = \"Age\", y = \"BMI\")\n```\n:::\n\n\n### Statistical Tests\n\nTo conduct a two-sample T-test (continuous variable by a binary variable), use the `t.test()` function with the `var.equal = FALSE` argument if desired:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(continuous_var ~ categorical_var, \n       data = dataset, var.equal = FALSE)\n```\n:::\n\n\nTo conduct a one-way ANOVA (test differences of two or more means), use the `aov()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(continuous_var ~ categorical_var, data = dataset)\n```\n:::\n\n\nTo conduct a non-parametric test Wilcoxon Rank-sum test (aka Mann-Whitney U test), use the `wilcox.test()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(continuous_var ~ categorical_var, data = dataset)\n```\n:::\n\n\n### Correlation coefficients\n\nOne can estimate correlation coefficients using the `cor()` function. The default is Pearson; one can estimate the Spearman correlation coefficients using the `method = \"spearman\"` argument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(dataset[, c(\"varname1\", \"varname2\", \"varname3\")], \n    method = \"spearman\")\n```\n:::\n\n\nYou can also provide two vectors if a single correlation is desired. For confidence intervals and hypothesis tests, use `cor.test()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(dataset$varname1, dataset$varname2)\n```\n:::\n\n\n# Regression analyses\n\n## Logistic Regression\n\n### Using continuous or binary variables\n\nFit a logistic regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- glm(outcome ~ exposure + var1 + var2, \n             data = dataset, family = binomial())\n```\n:::\n\n\nCode to output the beta coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(model)\n```\n:::\n\n\nCode to output the odds ratios:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(model))\n```\n:::\n\n\nCoefficients, standard errors, p-values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n:::\n\n\nCOnfidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(model)\n```\n:::\n\n\nThe `{broom}` package is helpful for getting model output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(model, exponentiate = TRUE, conf.int = TRUE)\n```\n:::\n\n\n### Using indicator variables\n\nR automatically creates indicator variables for factor or character variables (see previous section). To force it to create indicator variables for a numeric variable, you can use `factor()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- glm(outcome ~ exposure + factor(var1) + var2, \n             data = dataset, family = binomial())\n```\n:::\n\n\nTo change the referent group, you can use `fct_relevel()` to move a new level to be the first level (the other levels will stay in their original order, unless you include them):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset$newvar <- forcats::fct_relevel(dataset$oldvar, \"ref\"))\nmodel <- glm(outcome ~ exposure + newvar, \n             data = dataset, family = binomial())\n```\n:::\n\n\n### Stratified Logistic Regression\n\nTo fit a logistic regression model within levels of `categorical_var`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nmodels <- dataset |> \n  group_by(categorical_var) |> \n  group_map(~glm(outcome ~ exposure, data = .x))\n```\n:::\n\n\n## Linear Regression\n\nOne can conduct a linear regression model using either the `lm()` function or the `glm()` function with `family = gaussian()`. All of the same functions work on these models to extract model output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(outcome ~ exposure + var1 + var1)\nbroom::tidy(model, conf.int = TRUE)\n```\n:::\n\n\n## Survival Data\n\n### Set-up\n\nThe `{survival}` package is used for this entire section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\n```\n:::\n\n\nWithin that package, the `Surv()` function is used to \"create\" the outcome and is used in other functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSurv(time_to_outcome, outcome)\n```\n:::\n\n\n### Graphs\n\nKaplan-Meier curves overall:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm <- survfit(Surv(time_to_outcome, outcome) ~ 1, \n              data = dataset)\nplot(km)\n```\n:::\n\n\nKM curves by exposure groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm <- survfit(Surv(time_to_outcome, outcome) ~ exposure, \n              data = dataset)\nplot(km)\n```\n:::\n\n\nTo add 95% confidence intervals and censoring marks to the figure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(km, conf.int = TRUE, mark.time = TRUE)\n```\n:::\n\n\nLog negative-log survivor plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(km, fun = \"cloglog\")\n```\n:::\n\n\nThe `{survminer}` package is useful for creating survival curves for publication and can also include risk tables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvminer::ggsurvplot(km, risk.table = TRUE)\n```\n:::\n\n\n### Summary statistics\n\nTo see survivor function at various time points (ex. 30, 60, 90 days):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(km, times = c(30, 60, 90))\n```\n:::\n\n\nYou can also use the `tidy()` function to extract the estimates and confidence intervals over all times in a nicer format:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(km)\n```\n:::\n\n\nTo get median and other percentile survival times with confidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(km, probs = c(0.25, 0.5, 0.75))\n```\n:::\n\n\n### Statistical tests\n\nLog-rank test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvdiff(Surv(time_to_outcome, outcome) ~ exposure, \n         data = dataset)\n```\n:::\n\n\nSee the [`{survminer}` vignette](https://cran.r-project.org/web/packages/survminer/vignettes/Specifiying_weights_in_log-rank_comparisons.html#tharone-ware) for details on plotting survival curves with p-values from weighted log-rank tests (here, Tarone-Ware):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvmire::ggsurvplot(km, data = dataset, \n                     pval = TRUE, pval.method = TRUE,\n                     log.rank.weights = \"sqrtN\",\n                     pval.method.coord = c(3, 0.1),\n                     pval.method.size = 4)\n```\n:::\n\n\n### Cox Regression\n\nOne can conduct a Cox regression model using the `coxph()` function from the `{survival}` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncox <- coxph(Surv(time_to_outcome, outcome) ~ exposure, \n             data = dataset)\n```\n:::\n\n\nThe functions from the logistic regression section can be used to extract model output.\n\nCompeting Risk Analysis: In carrying out a competing risk analysis, the \"outcome\" variable can take on three values. There must be a value for the actual event of interest (outcome=1); a value for censoring (outcome=0); and a value for the competing risk event (outcome=2).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfg_data <- finegray(Surv(time_to_outcome, outcome) ~ exposure, \n                    data = dataset)\nfg_model <- coxph(Surv(fgstart, fgstop, fgstatus) ~ exposure, \n                  weight = fgwt, data = fg_data)\n```\n:::\n\n\n## Goodness of Fit Statistics\n\nTo get the log Likelihood/AIC/BIC for the most recent regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglik(model)\nAIC(model)\nBIC(model)\n```\n:::\n\n\nTo run a likelihood ratio test comparing two nested models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmtest::lrtest(mod_full, mod_nested)\n```\n:::\n",
    "supporting": [
      "cheat_sheet_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}